import csv
import logging
from pathlib import Path
from datetime import datetime

import sys
import subprocess
import pkg_resources
import time

import pandas as pd
from pathlib import Path

# Function to install a package if it's not already installed
def install_package(package):
    try:
        pkg_resources.get_distribution(package)
    except pkg_resources.DistributionNotFound:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_package ("openpyxl")

import numpy as np
import pandas as pd
import xgboost as xgb
import openpyxl
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, classification_report, confusion_matrix)
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from sklearn.pipeline import Pipeline

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Paths and file names
data_dir = Path(r"C:\Users\16036\OneDrive\Desktop\Investor Sight\Vulnerability")
stata_dir = data_dir
output_dir = data_dir
data_file = "Merged_dataset-2024-04-04.dta"

# Constants
TARGET_VAR = "F1_Factset_activism"
INDUSTRY_VAR = "sic_1_digit"
CLUSTER_ON = ["ln_mkv"]
CUTOFF_YEAR = 2015
MAX_MISSING = 0.25
MAX_CORRELATION = 0.85
MODEL_LABEL = "xgb"
NEIGHBORS = 100
DATA_FIELDS = [
    TARGET_VAR,
    INDUSTRY_VAR,
    "year",
    "gvkey",
    "ln_mkv",
    "activist_share_pct",
    "avg_networksize",
    "avg_support_director",
    "avg_support_management",
    "board_independent_ratio",
    "cash_holding",
    "ceo_appoint",
    "ceo_turnover",
    "ctype1",
    "ctype2",
    "ctype4",
    "ctype21",
    "current_ratio",
    "debt_ratio",
    "ded_pct",
    "instown_hhi",
    "intan_ratio",
    "ln_at",
    "ln_capx",
    "ln_rd",
    "ln_xad",
    "num_rec",
    "prstkc",
    "risk_taking_value",
    "roa",
    "roa_2",
    "roa_3",
    "roe",
    "roe_2",
    "roe_3",
    "short_ratio",
    "stock_ret",
    "tobin_q",
    "tra_pct",
    "w_environment",
    "w_governance",
    "w_social",
    "xsga_ratio",
    "yearfounded",
    "t_num_employees"
]

DATA_FIELDS = list(dict.fromkeys(DATA_FIELDS))
ID_FIELDS = DATA_FIELDS[:5]

# Hyperparameters for XGBoost
param_grid = {
    'colsample_bytree': 0.9,
    'gamma': 0.2,
    'learning_rate': 0.2,
    'max_depth': 3,
    'min_child_weight': 1,
    'n_estimators': 100,
    'reg_alpha': 0.1,
    'reg_lambda': 0,
    'subsample': 0.8
}

'''class DataProcessor:
    def __init__(self, data_file, data_fields, id_fields, max_missing, max_corr, cutoff_year=CUTOFF_YEAR):
        self.data_file = data_file
        self.data_fields = data_fields
        self.id_fields = id_fields
        self.max_missing = max_missing
        self.max_corr = max_corr
        self.cutoff_year = cutoff_year
        logging.info(f"DataProcessor initialized with {self.data_file}")

    def prepare_data(self, data):
        logging.info(f"Removing rows missing more than {self.max_missing * 100}% of data")
        start_rows = len(data)
        data = data.dropna(thresh=data.shape[1] * (1 - self.max_missing), axis=0)
        end_rows = len(data)
        logging.info(f"Dropped {start_rows - end_rows} rows")

        logging.info("Removing columns that are highly correlated")
        corr_matrix = data.corr(numeric_only=True).abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        columns_to_drop = [column for column in upper.columns if any(upper[column] > self.max_corr)]
        columns_to_drop = [c for c in columns_to_drop if c not in self.id_fields]
        data = data.drop(columns_to_drop, axis=1)
        for idx, c in enumerate(columns_to_drop):
            logging.info(f"{idx + 1:02d} Dropped column {c}")

        return data

    def load_data(self):
        logging.info(f"Reading {self.data_file} into a DataFrame")
        df = pd.read_stata(self.data_file)
        logging.info(f"Data loaded with {df.shape[0]} rows and {df.shape[1]} columns")

        df = df[self.data_fields]
        logging.info(f"Data filtered for {len(self.data_fields)} fields")

        logging.info(f"Data filtered for year >= {self.cutoff_year}")
        df = df[df["year"] >= self.cutoff_year]

        logging.info("Removing records with missing ID fields")
        start_rows = len(df)
        df = df.dropna(subset=self.id_fields)
        end_rows = len(df)
        logging.info(f"Dropped {start_rows - end_rows} rows")

        df = self.prepare_data(df)
        return df'''

class DataProcessor:
    def __init__(self, data_file, data_fields, id_fields, cutoff_year=CUTOFF_YEAR):
        self.data_file = data_file
        self.data_fields = data_fields
        self.id_fields = id_fields
        self.cutoff_year = cutoff_year
        logging.info(f"DataProcessor initialized with {self.data_file}")

    def prepare_data(self, data):
        # Log the initial data state
        start_rows = len(data)
        logging.info("Removing rows with missing 'ln_mkv'")

        # Drop rows where 'ln_mkv' is NaN
        data = data.dropna(subset=['ln_mkv'])
        
        # Log how many rows were dropped
        end_rows = len(data)
        logging.info(f"Dropped {start_rows - end_rows} rows due to missing 'ln_mkv'")

        return data

    def load_data(self):
        logging.info(f"Reading {self.data_file} into a DataFrame")
        df = pd.read_stata(self.data_file)
        logging.info(f"Data loaded with {df.shape[0]} rows and {df.shape[1]} columns")

        df = df[self.data_fields]
        logging.info(f"Data filtered for {len(self.data_fields)} fields")

        logging.info(f"Data filtered for year >= {self.cutoff_year}")
        df = df[df["year"] >= self.cutoff_year]

        logging.info("Preparation and filtering complete, proceeding without removing missing values.")
        df = self.prepare_data(df)
        return df


class DataMatcher(BaseEstimator, TransformerMixin):
    def __init__(self, target_var, id_fields, group_fields=[INDUSTRY_VAR, 'year'], cluster_on=['ln_mkv'], neighbors=1):
        self.target_var = target_var
        self.id_fields = id_fields
        self.group_fields = group_fields
        self.cluster_on = cluster_on
        self.nearest_neighbors = neighbors

    def create_groups(self, df):
        groups = df.groupby(self.group_fields, group_keys=False, observed=False)
        return groups

    def match_simple(self, group):
        logging.info(f"Matching in group with size: {len(group)}")
        minority = group[group[self.target_var] == 1]
        majority = group[group[self.target_var] == 0]

        if minority.empty or majority.empty:
            logging.info("Insufficient data for matching in this group.")
            return pd.DataFrame()

        nn = NearestNeighbors(n_neighbors=min(len(majority), self.nearest_neighbors))
        nn.fit(majority[self.cluster_on])
        distances, indices = nn.kneighbors(minority[self.cluster_on])
        matched_majority_indices = indices.flatten()

        matched_majority = majority.iloc[matched_majority_indices]
        matched_samples = pd.concat([minority, matched_majority]).drop_duplicates()

        logging.info(f"Matched samples size: {len(matched_samples)}")
        return matched_samples

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        groups = self.create_groups(X)
        matched_samples = pd.DataFrame()
        for name, group in groups:
            matched_group = self.match_simple(group)
            matched_samples = pd.concat([matched_samples, matched_group], ignore_index=True)
        logging.info(f"Total matched data size after processing all groups: {len(matched_samples)}")
        return matched_samples


class ClassificationReport:
    def __init__(self, model, model_step, X_test, y_test):
        self.model = model
        self.model_step = model_step
        self.X_test = X_test
        self.y_test = y_test
        self.predictions = model.predict(X_test)

    def value_counts(self):
        return self.y_test.value_counts()
    
    def total_points(self):
        return len(self.y_test)
    
    def count_ones(self):
        return (self.y_test == 1).sum()

    def accuracy_scores(self):
        return accuracy_score(self.y_test, self.predictions)

    def precision_scores_0(self):
        p = precision_score(self.y_test, self.predictions, average=None, zero_division=1)
        return p[0] if len(p) > 0 else None

    def precision_scores_1(self):
        p = precision_score(self.y_test, self.predictions, average=None, zero_division=1)
        return p[1] if len(p) > 1 else None

    def precision_scores_macro(self):
        return precision_score(self.y_test, self.predictions, average='macro', zero_division=1)

    def precision_scores_weighted(self):
        return precision_score(self.y_test, self.predictions, average='weighted', zero_division=1)

    def recall_scores_0(self):
        r = recall_score(self.y_test, self.predictions, average=None, zero_division=1)
        return r[0] if len(r) > 0 else None

    def recall_scores_1(self):
        r = recall_score(self.y_test, self.predictions, average=None, zero_division=1)
        return r[1] if len(r) > 1 else None

    def recall_scores_macro(self):
        return recall_score(self.y_test, self.predictions, average='macro', zero_division=1)

    def recall_scores_weighted(self):
        return recall_score(self.y_test, self.predictions, average='weighted', zero_division=1)

    def f1_scores_0(self):
        f = f1_score(self.y_test, self.predictions, average=None, zero_division=1)
        return f[0] if len(f) > 0 else None

    def f1_scores_1(self):
        f = f1_score(self.y_test, self.predictions, average=None, zero_division=1)
        return f[1] if len(f) > 1 else None

    def f1_scores_macro(self):
        return f1_score(self.y_test, self.predictions, average='macro', zero_division=1)

    def f1_scores_weighted(self):
        return f1_score(self.y_test, self.predictions, average='weighted', zero_division=1)

    def confusion_mat(self):
        return confusion_matrix(self.y_test, self.predictions)

    def feature_importance(self):
        classifier = self.model.named_steps[self.model_step]
        feature_importance = pd.Series(classifier.feature_importances_, index=self.X_test.columns)
        return feature_importance.sort_values(ascending=False)

    def generate_report(self, title, output_path):
        with open(output_path, 'a', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write(f"{title}\n")
            f.write("=" * 100 + "\n")
            f.write("Value Counts:\n")
            f.write(str(self.value_counts()))
            f.write("\n\nConfusion Matrix:\n")
            f.write(str(self.confusion_mat()))
            f.write("\n\nClassification Report:\n")
            f.write(str(classification_report(self.y_test, self.predictions, zero_division=1)))
            f.write("\n\nFeature Importance:\n")
            for feature, importance in self.feature_importance().items():
                f.write(f"{feature}: {importance: 6.4f}\n")
            f.write("\n\n")

    def record_results(self, title, output_path, industry, neighbors, train_total_points, train_count_ones):
        data = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'model': title,
            'industry': industry,
            'neighbors': neighbors,
            'precision 0': self.precision_scores_0(),
            'precision 1': self.precision_scores_1(),
            'macro average precision': self.precision_scores_macro(),
            'weighted average precision': self.precision_scores_weighted(),
            'recall 0': self.recall_scores_0(),
            'recall 1': self.recall_scores_1(),
            'macro average recall': self.recall_scores_macro(),
            'weighted average recall': self.recall_scores_weighted(),
            'f1 0': self.f1_scores_0(),
            'f1 1': self.f1_scores_1(),
            'macro average f1': self.f1_scores_macro(),
            'weighted average f1': self.f1_scores_weighted(),
            'train total points': train_total_points,
            'train count ones': train_count_ones,
            'test total points': self.total_points(),
            'test count ones': self.count_ones(),
            'accuracy': self.accuracy_scores()
        }

        df = pd.DataFrame([data])

        if not Path(output_path).exists():
            df.to_excel(output_path, index=False, engine='openpyxl')
        else:
            existing_df = pd.read_excel(output_path, engine='openpyxl')
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            combined_df.drop_duplicates(inplace=True)
            combined_df = combined_df.sort_values(by='f1 1', ascending=False)  # Sort by 'f1 1'
            combined_df.to_excel(output_path, index=False, engine='openpyxl')


def train_evaluate_model(train_features, test_features, train_labels, test_labels, pipeline, title, output_path, result_path, industry, neighbors):
    pipeline.fit(train_features, train_labels)

    # Create the ClassificationReport instance with test data
    report = ClassificationReport(pipeline, MODEL_LABEL, test_features, test_labels)
    report.generate_report(title, output_path)

    # Calculating training set metrics
    train_total_points = len(train_labels)
    train_count_ones = (train_labels == 1).sum()

    # Record results now also includes training set metrics
    report.record_results(title, result_path, industry, neighbors, train_total_points, train_count_ones)

def main():
    stata_file_path = stata_dir / data_file
    logging.info(f"Loading data from {stata_file_path}")
    df = DataProcessor(stata_file_path, DATA_FIELDS, ID_FIELDS, cutoff_year=CUTOFF_YEAR).load_data()

    # Separate the data from 2022 as the testing set
    test_data = df[df['year'] == 2021.0]
    train_data = df[df['year'] != 2021.0]

    result_path = output_dir / "model_resultsFacset2021.xlsx"
    pipeline = Pipeline([(MODEL_LABEL, xgb.XGBClassifier(**param_grid))])

    # Process each industry within the training data
    industries = train_data[INDUSTRY_VAR].unique()
    for industry in sorted(industries):
        logging.info(f"Running model on {industry} industry")
        df_industry = train_data[train_data[INDUSTRY_VAR] == industry]

        industry_test_data = test_data[test_data[INDUSTRY_VAR] == industry]
        test_labels = industry_test_data[TARGET_VAR]
        test_features = industry_test_data.drop([TARGET_VAR, INDUSTRY_VAR], axis=1)

        output_path = output_dir / f"{industry}_NeighborsFacset2021_report.txt"

        for n in range(1, NEIGHBORS + 1):
            logging.info(f"Creating matched dataset: {n} neighbors for {industry} industry")
            matched_train_data = DataMatcher(target_var=TARGET_VAR, id_fields=ID_FIELDS, cluster_on=CLUSTER_ON, neighbors=n).fit_transform(df_industry)

            if matched_train_data.empty:
                logging.warning(f"No matched data for {industry} industry with {n} neighbors")
                continue

            train_labels = matched_train_data[TARGET_VAR]
            train_features = matched_train_data.drop([TARGET_VAR, INDUSTRY_VAR], axis=1)

            title = f"XGBoost Model on {data_file} for {industry} (Neighbors={n})"
            try:
                train_evaluate_model(train_features, test_features, train_labels, test_labels, pipeline, title, output_path, result_path, industry, n)
            except Exception as e:
                logging.error(f"Error running model on {industry} industry with {n} neighbors: {e}")

    # Run the model on ALL industries after looping through each industry and neighbor
    for n in range(1, NEIGHBORS + 1):
        logging.info(f"Creating matched dataset: {n} neighbors for ALL industries")
        test_data = df[df['year'] == 2022.0]
        train_data = df[df['year'] != 2022.0]

        test_labels = test_data[TARGET_VAR]
        test_features = test_data.drop([TARGET_VAR, INDUSTRY_VAR], axis=1)

        matched_train_data = DataMatcher(target_var=TARGET_VAR, id_fields=ID_FIELDS, cluster_on=CLUSTER_ON, neighbors=n).fit_transform(train_data)

        if matched_train_data.empty:
            logging.warning(f"No matched data for ALL industries with {n} neighbors")
            continue

        train_labels = matched_train_data[TARGET_VAR]
        train_features = matched_train_data.drop([TARGET_VAR, INDUSTRY_VAR], axis=1)

        data_name = Path(data_file).stem
        title_all = f"XGBoost Model on {data_name} (Neighbors={n})"
        output_path_all = output_dir / f"{data_name}_NeighborsFacset2021_report.txt"

        try:
            train_evaluate_model(train_features, test_features, train_labels, test_labels, pipeline, title_all, output_path_all, result_path, "ALL", n)
        except Exception as e:
            logging.error(f"Error running model on ALL industries with {n} neighbors: {e}")

    # Paths
    data_dir = Path(r"C:\Users\16036\OneDrive\Desktop\Investor Sight\Vulnerability")
    model_results_path = data_dir / "model_resultsFacset2021.xlsx"
    summary_results_path = data_dir / "summary_resultsFacset2021.xlsx"

    # SIC 1 Digit mapping
    sic_mapping = {
        0: "Agriculture",
        1: "Mining",
        2: "Construction",
        3: "Manufacturing",
        4: "Transportation, Communications, Electric, Gas, and Sanitary Services",
        5: "Wholesale Trade",
        6: "Retail Trade",
        7: "Finance, Insurance, and Real Estate",
        8: "Services",
        9: "Public Administration",
        "ALL": "ALL"
    }

    # Read the model results
    df = pd.read_excel(model_results_path, engine='openpyxl')

    # Map SIC 1 Digit codes to descriptions
    df['SIC 1 digit'] = df['industry'].map(sic_mapping)

    # Group by industry and select the row with the highest f1 1 score for each industry
    best_rows = df.loc[df.groupby('industry')['f1 1'].idxmax()]

    # Calculate weighted average f1 1
    total_minority = best_rows['train count ones'].sum()
    best_rows['weighted_f1_1'] = best_rows['f1 1'] * best_rows['train count ones'] / total_minority
    weighted_avg_f1_1 = best_rows['weighted_f1_1'].sum()

    # Calculate average f1 1
    avg_f1_1 = best_rows['f1 1'].mean()

    # Create summary DataFrame
    summary_df = best_rows.drop(columns=['weighted_f1_1']).copy()

    # Format the summary DataFrame to look like the desired output
    summary_df = summary_df[['SIC 1 digit', 'f1 1', 'precision 1', 'recall 1', 'accuracy', 'train total points', 'train count ones', 'neighbors']]
    summary_df.columns = ['SIC 1 digit', 'Best f1 1', 'Precision 1', 'Recall 1', 'Accuracy', 'Total Train', 'Minority', 'Best Neighbors']

    # Display f1 1 in % with 4 significant digits
    summary_df['Best f1 1'] = summary_df['Best f1 1'].apply(lambda x: f"{x:.4%}")

    # Rank the summary DataFrame by high to low f1 1
    summary_df = summary_df.sort_values(by='Best f1 1', ascending=False)

    # Create a DataFrame for the weighted average and average f1 1
    summary_stats = pd.DataFrame({
        'SIC 1 digit': [''] * 3 + ['Weighted Avg f1 1', 'Avg f1 1'],
        'Best f1 1': [''] * 3 + [f"{weighted_avg_f1_1:.4%}", f"{avg_f1_1:.4%}"],
        'Precision 1': [''] * 5,
        'Recall 1': [''] * 5,
        'Accuracy': [''] * 5,
        'Total Train': [''] * 5,
        'Minority': [''] * 5,
        'Best Neighbors': [''] * 5
    })

    # Add explanation in the cell on the left
    summary_stats.loc[3, 'SIC 1 digit'] = 'Weighted average based on minority'
    summary_stats.loc[4, 'SIC 1 digit'] = 'Simple average of f1 1'

    # Concatenate the summary DataFrame and the summary stats DataFrame
    final_summary_df = pd.concat([summary_df, summary_stats], ignore_index=True)

    # Write summary to a new Excel file
    final_summary_df.to_excel(summary_results_path, index=False, engine='openpyxl')


if __name__ == "__main__":
    main()

